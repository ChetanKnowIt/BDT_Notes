{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMDpK9ur0+tlL7pSx+IHGFI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChetanKnowIt/BDT_Notes/blob/main/HBase_Pyspark_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr />\n",
        "<h1 align=\"center\" > PySpark‚≠ê HBaseüê¨ connectivity </h1>\n",
        "\n",
        "<hr />"
      ],
      "metadata": {
        "id": "d44MrVreY4mt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Requirements/Prerequisites: \n",
        "\n",
        "  1. Fully Distributed mode installation (Multinode Hadoop environment + HBase installation) from here [Guru99 installtion link](https://www.guru99.com/hbase-installation-guide.html) \n",
        "  2. pip install pyspark \n",
        "  3. pip install happybase\n",
        "  4. pip install Flask\n",
        "\n",
        "<hr />\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TywY6V_fZMV2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyspark"
      ],
      "metadata": {
        "id": "SHnLDl-DK5jm",
        "outputId": "a3877c6c-6f08-4524-9d27-c9a863c09aae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.2.tar.gz (281.4 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.2-py2.py3-none-any.whl size=281824025 sha256=df0fda8b25a6f7c2a09bdcee18495e6a18dbb9a1caa305e20d5176bdd482a415\n",
            "  Stored in directory: /root/.cache/pip/wheels/b1/59/a0/a1a0624b5e865fd389919c1a10f53aec9b12195d6747710baf\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install findspark"
      ],
      "metadata": {
        "id": "ZCp8PpOULP_W",
        "outputId": "3c70cde2-92c4-4deb-ca51-c781caf544c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "Zc2YHqzyK-2Z"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!which python3\n"
      ],
      "metadata": {
        "id": "2Stk4AC_MARH",
        "outputId": "5fd09424-d1cd-49ea-981e-36045b656fe8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/bin/python3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "print(sys.executable)\n"
      ],
      "metadata": {
        "id": "US85EK4oMJcu",
        "outputId": "880ec7a9-6cad-4a90-88e5-1f6d1e3d798a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/bin/python3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('myAppName') \\\n",
        "    .config('spark.executorEnv.PYTHON_EXECUTABLE', '/usr/bin/python3') \\\n",
        "    .getOrCreate()\n",
        "sc = spark.sparkContext\n"
      ],
      "metadata": {
        "id": "IbtHKZ-cMZPR"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sc.getConf().get('spark.executorEnv.PYTHON_EXECUTABLE'))"
      ],
      "metadata": {
        "id": "HBz32udpMllO",
        "outputId": "25a6a0ef-c855-4946-c609-de3c516a0a38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "python_exec = sc.getConf().get('spark.executorEnv.PYTHON_EXECUTABLE')\n",
        "print(python_exec)"
      ],
      "metadata": {
        "id": "U-WF196QNNZ3",
        "outputId": "f8805fff-afc8-4dc3-d1ac-7cdee790b290",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"my_app\") \\\n",
        "    .config(\"spark.driver.memory\", \"8g\") \\\n",
        "    .config(\"spark.executor.memory\", \"8g\") \\\n",
        "    .getOrCreate()\n"
      ],
      "metadata": {
        "id": "ni2pzvERQjj7"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Flow: \n"
      ],
      "metadata": {
        "id": "DB3gHnSUaC3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "Image(url=\"https://drive.google.com/u/0/uc?id=12AWXzV0JAdP0uZeo7lrdt6_v3Po19bL9\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "id": "0QPleMP_cNOt",
        "outputId": "e3737e66-4f1c-4596-d084-90872f8c3f3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://drive.google.com/u/0/uc?id=12AWXzV0JAdP0uZeo7lrdt6_v3Po19bL9\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. We can start with reading file from HDFS, processing "
      ],
      "metadata": {
        "id": "kq0OajPGarWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyarrow as pa\n",
        "import pyarrow.csv as csv\n",
        "import pandas as pd\n",
        "\n",
        "# Define the HDFS file path\n",
        "hdfs_path = 'hdfs://my_cluster:8020/my_file.csv'\n",
        "\n",
        "# Create a PyArrow HDFS filesystem object\n",
        "fs = pa.hdfs.connect()\n",
        "\n",
        "# Open the HDFS file as a memory buffer\n",
        "with fs.open(hdfs_path) as file:\n",
        "    buffer = pa.BufferReader(file.read())\n",
        "\n",
        "# Read the CSV data from the memory buffer using PyArrow\n",
        "table = csv.read_csv(buffer)\n",
        "\n",
        "# Convert the PyArrow Table to a Pandas DataFrame\n",
        "data = table.to_pandas()\n"
      ],
      "metadata": {
        "id": "JgMoRXhVfobn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### and dumping on HBase from pandas \n",
        " https://happybase.readthedocs.io/en/latest/user.html example table.put('2',{'f1': 'hey'})"
      ],
      "metadata": {
        "id": "hiEUfskPftQ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import happybase\n",
        "import pandas as pd\n",
        "\n",
        "# Load data into a Pandas DataFrame\n",
        "data = pd.read_csv('my_data.csv')\n",
        "\n",
        "# Create a connection object to HBase\n",
        "connection = happybase.Connection('localhost', port=9090)\n",
        "\n",
        "# Create a table object in Happybase\n",
        "table_name = 'my_table'\n",
        "column_family = 'my_cf'\n",
        "table = connection.table(table_name)\n",
        "\n",
        "# Iterate over the rows in the Pandas DataFrame\n",
        "for _, row in data.iterrows():\n",
        "    # Extract data from the row\n",
        "    row_key = row['my_key_column']\n",
        "    data_dict = {\n",
        "        f'{column_family}:{column}': str(row[column])\n",
        "        for column in data.columns\n",
        "        if column != 'my_key_column'\n",
        "    }\n",
        "    \n",
        "    # Write the row to HBase\n",
        "    table.put(row_key, data_dict)\n"
      ],
      "metadata": {
        "id": "9nx9gERieTlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. then we retrieve values with this example below"
      ],
      "metadata": {
        "id": "vo0yPgYCeV8k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import happybase\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a HBase connection object\n",
        "connection = happybase.Connection('localhost', port=9090)\n",
        "\n",
        "# Create a table object\n",
        "table = connection.table('my_table')\n",
        "\n",
        "# Retrieve data from HBase table\n",
        "data = []\n",
        "for key, values in table.scan():\n",
        "    row = {}\n",
        "    row['key'] = key\n",
        "    for column, value in values.items():\n",
        "        row[column.decode('utf-8')] = value.decode('utf-8')\n",
        "    data.append(row)\n",
        "\n",
        "# Convert HBase data to PySpark DataFrame\n",
        "spark = SparkSession.builder.appName('HBase to PySpark').getOrCreate()\n",
        "df = spark.createDataFrame(data)\n",
        "\n",
        "# Print PySpark DataFrame\n",
        "df.show()\n"
      ],
      "metadata": {
        "id": "pjk9NO58YrNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. we use this spark dataframe for model training and evaluation \n",
        "### 4. we dump data back to HBase with this example below"
      ],
      "metadata": {
        "id": "jhQpyceiaRjE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import happybase\n",
        "import pickle\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Train a logistic regression model in Spark\n",
        "spark = SparkSession.builder.appName('Logistic Regression').getOrCreate()\n",
        "data = spark.read.csv('my_data.csv', header=True, inferSchema=True)\n",
        "assembler = VectorAssembler(inputCols=['feature1', 'feature2'], outputCol='features')\n",
        "data = assembler.transform(data)\n",
        "train, test = data.randomSplit([0.7, 0.3])\n",
        "lr = LogisticRegression()\n",
        "model = lr.fit(train)\n",
        "predictions = model.transform(test)\n",
        "evaluator = BinaryClassificationEvaluator()\n",
        "auc = evaluator.evaluate(predictions)\n",
        "\n",
        "# Serialize the results\n",
        "results = {'auc': auc}\n",
        "serialized_results = pickle.dumps(results)\n",
        "\n",
        "# Create a connection object to HBase\n",
        "connection = happybase.Connection('localhost', port=9090)\n",
        "\n",
        "# Create a table object in Happybase\n",
        "table_name = 'my_table'\n",
        "table = connection.table(table_name)\n",
        "\n",
        "# Write the results to HBase\n",
        "row_key = 'my_row'\n",
        "column_family = 'results'\n",
        "column_qualifier = 'logistic_regression'\n",
        "table.put(row_key, {f'{column_family}:{column_qualifier}': serialized_results})\n"
      ],
      "metadata": {
        "id": "klLlHdYdYtVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Finally we can use Flask to render this table onto an HTML for the web"
      ],
      "metadata": {
        "id": "RZbv64DGgwDg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import happybase\n",
        "from flask import Flask, render_template\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route('/')\n",
        "def index():\n",
        "    # Create a connection object to HBase\n",
        "    connection = happybase.Connection('localhost', port=9090)\n",
        "\n",
        "    # Create a table object in Happybase\n",
        "    table_name = 'my_table'\n",
        "    column_family = 'my_cf'\n",
        "    table = connection.table(table_name)\n",
        "\n",
        "    # Execute a scan operation to read data from HBase\n",
        "    scan_results = table.scan()\n",
        "\n",
        "    # Convert the scan results to a list of dictionaries\n",
        "    data = []\n",
        "    for key, values in scan_results:\n",
        "        row_dict = {f'{column_family}:{column}': value.decode('utf-8') for column, value in values.items()}\n",
        "        row_dict['row_key'] = key.decode('utf-8')\n",
        "        data.append(row_dict)\n",
        "\n",
        "    # Render the results in an HTML table using Jinja templates\n",
        "    return render_template('index.html', data=data)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(debug=True)\n"
      ],
      "metadata": {
        "id": "pqTIq_f0g6fU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!doctype html>\n",
        "<html>\n",
        "  <head>\n",
        "    <title>HBase Data</title>\n",
        "  </head>\n",
        "  <body>\n",
        "    <h1>HBase Data</h1>\n",
        "    <table>\n",
        "      <thead>\n",
        "        <tr>\n",
        "          <th>Row Key</th>\n",
        "          <th>Column 1</th>\n",
        "          <th>Column 2</th>\n",
        "          <th>Column 3</th>\n",
        "        </tr>\n",
        "      </thead>\n",
        "      <tbody>\n",
        "        {% for row in data %}\n",
        "          <tr>\n",
        "            <td>{{ row['row_key'] }}</td>\n",
        "            <td>{{ row['my_cf:column1'] }}</td>\n",
        "            <td>{{ row['my_cf:column2'] }}</td>\n",
        "            <td>{{ row['my_cf:column3'] }}</td>\n",
        "          </tr>\n",
        "        {% endfor %}\n",
        "      </tbody>\n",
        "    </table>\n",
        "  </body>\n",
        "</html>\n"
      ],
      "metadata": {
        "id": "fPCxaKIkg_ha"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8-rd277_hATF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}